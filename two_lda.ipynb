{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "from datetime import datetime\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from metrics import utils\n",
    "import numpy as np\n",
    "from lda import LDA\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_string_gen(file_dir, words):\n",
    "    l=[]\n",
    "\n",
    "    with open(file_dir, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        for word in words:\n",
    "            if word in line:\n",
    "                l.append(re.search(r'\\d+', line).group())\n",
    "    l = list(dict.fromkeys(l))\n",
    "\n",
    "    string = \"\"\n",
    "    for i,j in enumerate(l):\n",
    "        if i==len(l)-1:\n",
    "            string += f\"Topic == {j}\"\n",
    "        else:\n",
    "            string += f\"Topic == {j} | \"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Current Dataset:\n",
      "AP: 6623 Articles\n",
      "Fox: 5346 Articles\n",
      "CNN: 3205 Articles\n",
      "ABC: 1794 Articles\n",
      "CBS: 3784 Articles\n",
      "NYT: 3148 Articles\n",
      "Mirror: 2053 Articles\n",
      "Reuters: 18213 Articles\n",
      "Express: 10804 Articles\n",
      "HuffPost: 682 Articles\n",
      "Guardian: 5293 Articles\n",
      "DailyMail: 8239 Articles\n",
      "-> Saved CSV with 69184 articles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.unite_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTR:\n",
    "    def __init__(self) -> None:\n",
    "        self.sources = [\"AP\", \"Fox\", \"CNN\", \"ABC\", \"CBS\", \"NYT\", \"Mirror\", \"Reuters\", \"Express\", \"HuffPost\", \"Guardian\", \"DailyMail\"]\n",
    "\n",
    "    def kld_window(self, dataframe, date_start, date_end, kld_days_window):\n",
    "        data = dataframe\n",
    "        data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "        df_split = data.loc[(data[\"Date\"] >= date_start) & (data[\"Date\"] < date_end)]\n",
    "        df_count = df_split.resample(\"D\", on=\"Date\").apply({\"URL\": \"count\"})\n",
    "        daily_count = int(sum(df_count[\"URL\"].tolist()) / len(df_count[\"URL\"].tolist()))\n",
    "        print(f\"-> This dataset has an average of {daily_count} daily stories from {date_start} to {date_end}.\")\n",
    "        print(f\"-> KLD window will be of {kld_days_window}*{daily_count} = {kld_days_window*daily_count} articles.\\n\")\n",
    "        return kld_days_window * daily_count\n",
    "\n",
    "    def learn_topics(self, dataframe, topicnum, vocabsize, num_iter):\n",
    "        # Removes stopwords\n",
    "        texts = dataframe[\"Text\"].tolist()\n",
    "        texts_no_sw = []\n",
    "        for text in texts:\n",
    "            text_no_sw = remove_stopwords(text)\n",
    "            texts_no_sw.append(text_no_sw)\n",
    "\n",
    "        # Get vocab and word counts. Use the top 10k most frequent\n",
    "        # lowercase unigrams with at least 2 alphabetical, non-numeric characters,\n",
    "        # punctuation treated as separators.\n",
    "        texts = texts_no_sw\n",
    "        count_vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b[^\\W\\d]{2,}\\b\", max_features=vocabsize, lowercase=True)\n",
    "        doc_vcnts = count_vectorizer.fit_transform(texts)\n",
    "        vocabulary = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Learn topics.\n",
    "        lda_model = LDA(topicnum, n_iter=num_iter, refresh=100)\n",
    "        doc_topic = lda_model.fit_transform(doc_vcnts)\n",
    "        topic_word = lda_model.topic_word_\n",
    "\n",
    "        return doc_topic, topic_word, vocabulary\n",
    "\n",
    "    def save_topicmodel(self, path, doc_topic, topic_word, vocabulary, source):\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        topicmixture_outpath = os.path.join(path, source + \"_TopicMixtures.txt\")\n",
    "        np.savetxt(topicmixture_outpath, doc_topic)\n",
    "        topic_outpath = os.path.join(path, source + \"_Topics.txt\")\n",
    "        np.savetxt(topic_outpath, topic_word)\n",
    "        vocab_outpath = os.path.join(path, source + \"_Vocab.txt\")\n",
    "        with open(vocab_outpath, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "            for word in vocabulary:\n",
    "                file.write(word + \"\\n\")\n",
    "\n",
    "        return topicmixture_outpath, topic_outpath, vocab_outpath\n",
    "\n",
    "    def kld_from_probdists(self, pdists0, pdists1):\n",
    "\n",
    "        assert pdists0.shape == pdists1.shape, \"pdist* shapes must be identical\"\n",
    "        if len(pdists0.shape) == 1:\n",
    "            kl_divergences = (pdists1 * np.log2(pdists1 / pdists0)).sum()\n",
    "        elif len(pdists0.shape) == 2:\n",
    "            kl_divergences = (pdists1 * np.log2(pdists1 / pdists0)).sum(axis=1)\n",
    "\n",
    "        return kl_divergences\n",
    "\n",
    "    def novelty_transience_resonance(self, thetas_arr, scale):\n",
    "\n",
    "        speechstart = scale\n",
    "        speechend = thetas_arr.shape[0] - scale\n",
    "        novelties = []\n",
    "        transiences = []\n",
    "        resonances = []\n",
    "        for j in range(speechstart, speechend, 1):\n",
    "            center_theta = thetas_arr[j]\n",
    "            after_boxend = j + scale + 1\n",
    "            before_boxstart = j - scale\n",
    "            before_theta_arr = thetas_arr[before_boxstart:j]\n",
    "            beforenum = before_theta_arr.shape[0]\n",
    "            before_centertheta_arr = np.tile(center_theta, reps=(beforenum, 1))\n",
    "            after_theta_arr = thetas_arr[j + 1 : after_boxend]\n",
    "            afternum = after_theta_arr.shape[0]\n",
    "            after_centertheta_arr = np.tile(center_theta, reps=(afternum, 1))\n",
    "            before_klds = self.kld_from_probdists(before_theta_arr, before_centertheta_arr)\n",
    "            after_klds = self.kld_from_probdists(after_theta_arr, after_centertheta_arr)\n",
    "            novelty = np.mean(before_klds)\n",
    "            transience = np.mean(after_klds)\n",
    "            novelties.append(novelty)\n",
    "            transiences.append(transience)\n",
    "            resonances.append(novelty - transience)\n",
    "        for _ in range(0, scale):\n",
    "            transiences.insert(0, 0)\n",
    "            transiences.append(0)\n",
    "            novelties.insert(0, 0)\n",
    "            novelties.append(0)\n",
    "            resonances.insert(0, 0)\n",
    "            resonances.append(0)\n",
    "\n",
    "        return novelties, transiences, resonances\n",
    "\n",
    "    def save_novel_trans_reson(self, path, novelties, transiences, resonances, source):\n",
    "\n",
    "        outpath = os.path.join(path, source + \"_NovelTransReson.txt\")\n",
    "        np.savetxt(outpath, np.vstack(list(zip(novelties, transiences, resonances))))\n",
    "\n",
    "    def routine(self, date_start, date_end, kld_days_window, vocabsize, num_iter):\n",
    "        \n",
    "        print(\"-> Reading first topic modeling results (LDA)...\")\n",
    "        results = pd.read_csv(os.path.join(ROOT_DIR,\"results\",\"All_Results.csv\"))\n",
    "        query_string = query_string_gen(os.path.join(ROOT_DIR,\"results\",\"All_TopicsWords.txt\"), \n",
    "            words=[\"drones\", \"troops\", \"strike\", \"killed\", \"attack\", \"shelling\", \"strikes\"]\n",
    "            )\n",
    "        print(f\"-> Conflict-related topics on first LDA analysis: {query_string}\\n\")\n",
    "        results = results.query(query_string)\n",
    "        results = results.drop(columns=[\"Novelty\",\"Transience\",\"Resonance\",\"Comments\"])\n",
    "\n",
    "        source_dfs = []\n",
    "        for i, source in enumerate(self.sources):\n",
    "            query = results.query(f\"Source == '{str(source)}'\")\n",
    "            source_dfs.append(query.copy())\n",
    "            print(f\"{source} length: {len(query)}\")\n",
    "        results[\"Source\"] = \"All\"\n",
    "        source_dfs.append(results.copy())\n",
    "        print(f\"All length: {len(results)}\\n\")\n",
    "\n",
    "\n",
    "        for i, j in enumerate(source_dfs):\n",
    "            data = source_dfs[i]\n",
    "            source = data.at[data.index[0], 'Source']\n",
    "            print(f\"-> Starting {source} second topic modeling (LDA)...\")\n",
    "            scale = self.kld_window(data, date_start, date_end, kld_days_window)\n",
    "\n",
    "            doc_topic, topic_word, vocabulary = self.learn_topics(data, 30, vocabsize, num_iter)\n",
    "\n",
    "            # getting topic of each text\n",
    "            topics = []\n",
    "            for i in range(len(data)):\n",
    "                topics.append(doc_topic[i].argmax())\n",
    "\n",
    "            self.save_topicmodel(os.path.join(ROOT_DIR, \"results_two_lda\"), doc_topic, topic_word, vocabulary, source)\n",
    "            novelties, transiences, resonances = self.novelty_transience_resonance(doc_topic, scale)\n",
    "            self.save_novel_trans_reson(os.path.join(ROOT_DIR, \"results_two_lda\"), novelties, transiences, resonances, source)\n",
    "            ntr_data = data\n",
    "            ntr_data[\"Novelty\"] = novelties\n",
    "            ntr_data[\"Transience\"] = transiences\n",
    "            ntr_data[\"Resonance\"] = resonances\n",
    "            ntr_data[\"Topic2\"] = topics\n",
    "            ntr_data.to_csv(os.path.join(ROOT_DIR, \"results_two_lda\", source + \"_Results.csv\"), index=False)\n",
    "\n",
    "            # geting words of each topic\n",
    "            words = []\n",
    "            for i, topic_dist in enumerate(topic_word):\n",
    "                topic_words = np.array(vocabulary)[np.argsort(topic_dist)][:-16:-1]\n",
    "                words.append(f\"Topic {i}: {' '.join(topic_words)}\")\n",
    "            with open(os.path.join(ROOT_DIR, \"results_two_lda\", source + \"_TopicsWords.txt\"), \"w\") as file:\n",
    "                file.write(\"\\n\".join(map(str, words)))\n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "        print(\"-> All LDA data saved.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Reading first topic modeling results (LDA)...\n",
      "-> Conflict-related topics on first LDA analysis: Topic == 4 | Topic == 7 | Topic == 20 | Topic == 45 | Topic == 47 | Topic == 49\n",
      "\n",
      "AP length: 663\n",
      "Fox length: 938\n",
      "CNN length: 515\n",
      "ABC length: 260\n",
      "CBS length: 543\n",
      "NYT length: 596\n",
      "Mirror length: 623\n",
      "Reuters length: 2736\n",
      "Express length: 3004\n",
      "HuffPost length: 81\n",
      "Guardian length: 767\n",
      "DailyMail length: 1758\n",
      "All length: 12484\n",
      "\n",
      "-> Starting AP second topic modeling (LDA)...\n",
      "-> This dataset has an average of 1 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*1 = 2 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 663\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 522391\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n",
      "INFO:lda:<0> log likelihood: -6207189\n",
      "INFO:lda:<100> log likelihood: -4255133\n",
      "INFO:lda:<200> log likelihood: -4220150\n",
      "INFO:lda:<299> log likelihood: -4204303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting Fox second topic modeling (LDA)...\n",
      "-> This dataset has an average of 3 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*3 = 6 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 938\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 300467\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n",
      "INFO:lda:<0> log likelihood: -3676800\n",
      "INFO:lda:<100> log likelihood: -2479846\n",
      "INFO:lda:<200> log likelihood: -2460378\n",
      "INFO:lda:<299> log likelihood: -2450753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting CNN second topic modeling (LDA)...\n",
      "-> This dataset has an average of 1 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*1 = 2 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 515\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 270644\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n",
      "INFO:lda:<0> log likelihood: -3300390\n",
      "INFO:lda:<100> log likelihood: -2231738\n",
      "INFO:lda:<200> log likelihood: -2212356\n",
      "INFO:lda:<299> log likelihood: -2202899\n",
      "INFO:lda:n_documents: 260\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 117162\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting ABC second topic modeling (LDA)...\n",
      "-> This dataset has an average of 0 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*0 = 0 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:<0> log likelihood: -1481115\n",
      "INFO:lda:<100> log likelihood: -975691\n",
      "INFO:lda:<200> log likelihood: -965249\n",
      "INFO:lda:<299> log likelihood: -962364\n",
      "INFO:lda:n_documents: 543\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 195211\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting CBS second topic modeling (LDA)...\n",
      "-> This dataset has an average of 1 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*1 = 2 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:<0> log likelihood: -2438649\n",
      "INFO:lda:<100> log likelihood: -1622772\n",
      "INFO:lda:<200> log likelihood: -1608721\n",
      "INFO:lda:<299> log likelihood: -1602729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting NYT second topic modeling (LDA)...\n",
      "-> This dataset has an average of 1 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*1 = 2 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 596\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 382326\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n",
      "INFO:lda:<0> log likelihood: -4658176\n",
      "INFO:lda:<100> log likelihood: -3196102\n",
      "INFO:lda:<200> log likelihood: -3170519\n",
      "INFO:lda:<299> log likelihood: -3159295\n",
      "INFO:lda:n_documents: 623\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 198041\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting Mirror second topic modeling (LDA)...\n",
      "-> This dataset has an average of 3 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*3 = 6 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:<0> log likelihood: -2427817\n",
      "INFO:lda:<100> log likelihood: -1660150\n",
      "INFO:lda:<200> log likelihood: -1645075\n",
      "INFO:lda:<299> log likelihood: -1641339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting Reuters second topic modeling (LDA)...\n",
      "-> This dataset has an average of 8 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*8 = 16 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 2736\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 463909\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n",
      "INFO:lda:<0> log likelihood: -5621162\n",
      "INFO:lda:<100> log likelihood: -3766377\n",
      "INFO:lda:<200> log likelihood: -3736863\n",
      "INFO:lda:<299> log likelihood: -3728293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting Express second topic modeling (LDA)...\n",
      "-> This dataset has an average of 9 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*9 = 18 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 3004\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 857281\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n",
      "INFO:lda:<0> log likelihood: -10178323\n",
      "INFO:lda:<100> log likelihood: -7187780\n",
      "INFO:lda:<200> log likelihood: -7122041\n",
      "INFO:lda:<299> log likelihood: -7098328\n",
      "INFO:lda:n_documents: 81\n",
      "INFO:lda:vocab_size: 5199\n",
      "INFO:lda:n_words: 24393\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n",
      "INFO:lda:<0> log likelihood: -310446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting HuffPost second topic modeling (LDA)...\n",
      "-> This dataset has an average of 0 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*0 = 0 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:<100> log likelihood: -205362\n",
      "INFO:lda:<200> log likelihood: -204087\n",
      "INFO:lda:<299> log likelihood: -203376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting Guardian second topic modeling (LDA)...\n",
      "-> This dataset has an average of 2 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*2 = 4 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 767\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 363801\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n",
      "INFO:lda:<0> log likelihood: -4445640\n",
      "INFO:lda:<100> log likelihood: -3072140\n",
      "INFO:lda:<200> log likelihood: -3048979\n",
      "INFO:lda:<299> log likelihood: -3041124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting DailyMail second topic modeling (LDA)...\n",
      "-> This dataset has an average of 6 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*6 = 12 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 1758\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 1655278\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n",
      "INFO:lda:<0> log likelihood: -19240875\n",
      "INFO:lda:<100> log likelihood: -14072162\n",
      "INFO:lda:<200> log likelihood: -13932201\n",
      "INFO:lda:<299> log likelihood: -13882973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Starting All second topic modeling (LDA)...\n",
      "-> This dataset has an average of 40 daily stories from 2022-03-01 to 2022-08-01.\n",
      "-> KLD window will be of 2*40 = 80 articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 12484\n",
      "INFO:lda:vocab_size: 10000\n",
      "INFO:lda:n_words: 5277969\n",
      "INFO:lda:n_topics: 30\n",
      "INFO:lda:n_iter: 300\n",
      "INFO:lda:<0> log likelihood: -60545641\n",
      "INFO:lda:<100> log likelihood: -44574670\n",
      "INFO:lda:<200> log likelihood: -44288865\n",
      "INFO:lda:<299> log likelihood: -44213028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> All LDA data saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NTR().routine(date_start=\"2022-03-01\", date_end=\"2022-08-01\", kld_days_window=2, vocabsize=10000, num_iter=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_pearson(dir,source,filter=False):\n",
    "    events = pd.read_csv(os.path.join(ROOT_DIR,\"data\",\"Ukraine_Black_Sea_2020_2022_Nov18.csv\"), parse_dates=[\"EVENT_DATE\"])\n",
    "    events = events[[\"EVENT_DATE\",\"EVENT_TYPE\",\"FATALITIES\"]].rename(columns={\"EVENT_DATE\":\"Date\",\"EVENT_TYPE\":\"Count\",\"FATALITIES\":\"Fatalities\"})  # pegando somente colunas relevantes\n",
    "    events = events.set_index(\"Date\") # convertendo coluna de datas pra datetime e setando indice\n",
    "    events = events.resample('D').agg({'Count': 'count', 'Fatalities': 'sum'}) # resample: contagem de eventos, soma de fatalidades\n",
    "\n",
    "    results = pd.read_csv(os.path.join(dir,f\"{source}_Results.csv\"), parse_dates=[\"Date\"],index_col=[\"Date\"])\n",
    "    if filter == True:\n",
    "        results = results.query(query_string_gen(os.path.join(dir,f\"{source}_TopicsWords.txt\"), words=[\"missiles\", \"strike\", \"killed\", \"attack\", \"shelling\", \"missile\"]))\n",
    "    results = results[[\"Resonance\",\"Novelty\",\"Transience\"]]\n",
    "    results = results.sort_index()\n",
    "    results = results.loc[\"2018-01-01\":\"2022-11-18\"] # Matching other dataframe\n",
    "    results = results.resample('D').sum()\n",
    "    results[[\"Count\",\"Fatalities\"]] = events[[\"Count\",\"Fatalities\"]].copy()\n",
    "    results = results.loc[\"2020-01-01\":\"2022-11-16\"] \n",
    "\n",
    "    s_fatalities, s_fatalities_p = spearmanr(results['Fatalities'], results['Resonance'])\n",
    "    p_fatalities, p_fatalities_p = pearsonr(results['Fatalities'], results['Resonance'])\n",
    "    s_events, s_events_p = spearmanr(results['Count'], results['Resonance'])\n",
    "    p_events, p_events_p = pearsonr(results['Count'], results['Resonance'])\n",
    "\n",
    "    if s_fatalities_p <= 0.05:\n",
    "        print(f'{source} (Filter = {filter}) R X FAT: Spearman: {s_fatalities:.4f} p-value: {s_fatalities_p:.4f}')\n",
    "    if p_fatalities_p <= 0.05:\n",
    "        print(f'{source} (Filter = {filter}) R X FAT: Pearson: {p_fatalities:.4f} p-value: {p_fatalities_p:.4f}')\n",
    "    if s_events_p <= 0.05:\n",
    "        print(f'{source} (Filter = {filter}) R X EVE: Spearman: {s_events:.4f} p-value: {s_events_p:.4f}')\n",
    "    if p_events_p <= 0.05:\n",
    "        print(f'{source} (Filter = {filter}) R X EVE: Pearson: {p_events:.4f} p-value: {p_events_p:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP (Filter = False) R X EVE: Spearman: 0.0649 p-value: 0.0364\n",
      "ABC (Filter = True) R X FAT: Spearman: 0.0606 p-value: 0.0497\n",
      "ABC (Filter = True) R X EVE: Spearman: 0.0755 p-value: 0.0144\n",
      "ABC (Filter = True) R X EVE: Pearson: 0.0628 p-value: 0.0419\n",
      "Express (Filter = True) R X EVE: Pearson: -0.0951 p-value: 0.0034\n",
      "HuffPost (Filter = True) R X FAT: Spearman: 0.0932 p-value: 0.0030\n",
      "DailyMail (Filter = False) R X EVE: Spearman: -0.0649 p-value: 0.0389\n"
     ]
    }
   ],
   "source": [
    "sources = [\"AP\", \"Fox\", \"CNN\", \"ABC\", \"CBS\", \"NYT\", \"Mirror\", \"Reuters\", \"Express\", \"HuffPost\", \"Guardian\", \"DailyMail\", \"All\"]\n",
    "for source in sources:\n",
    "    for i in [True,False]:\n",
    "        spearman_pearson(os.path.join(ROOT_DIR,\"results\"),source,i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (Filter = False) R X FAT: Pearson: 0.0769 p-value: 0.0126\n",
      "CBS (Filter = False) R X FAT: Pearson: 0.0748 p-value: 0.0157\n",
      "DailyMail (Filter = False) R X FAT: Pearson: 0.0968 p-value: 0.0024\n"
     ]
    }
   ],
   "source": [
    "for source in sources:\n",
    "    spearman_pearson(os.path.join(ROOT_DIR,\"results_two_lda\"),source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71a723942456804a71d025442f2ccd3a5c8db2153e1c9e51f0af23a7e755532d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
