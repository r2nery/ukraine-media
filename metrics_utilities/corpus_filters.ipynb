{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\"AP\", \"Fox\", \"CNN\", \"ABC\", \"CBS\", \"NYT\", \"Mirror\", \"Reuters\", \"Express\", \"Guardian\", \"DailyMail\"]\n",
    "for source in sources:\n",
    "    data = pd.read_csv(os.path.join(ROOT_DIR,\"data\",f\"{source}.csv\"),parse_dates=[\"Date\"],index_col=\"Date\")\n",
    "    data = data.sort_index().loc[\"2022-01-01\":\"2022-12-31\"]\n",
    "    data.to_csv(os.path.join(ROOT_DIR,\"data\",f\"{source}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_string_gen(file):\n",
    "    l=[]\n",
    "\n",
    "    with open(os.path.join(ROOT_DIR,\"results_0\",f\"{file}_TopicsWords.txt\"), 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        #print(line[0:1])\n",
    "        if line[0:1] != \"#\":\n",
    "            l.append(re.search(r'\\d+', line).group())\n",
    "    l = list(dict.fromkeys(l))\n",
    "\n",
    "    string = \"\"\n",
    "    for i,j in enumerate(l):\n",
    "        if i==len(l)-1:\n",
    "            string += f\"Topic == {j}\"\n",
    "        else:\n",
    "            string += f\"Topic == {j} | \"\n",
    "    return string\n",
    "\n",
    "def filter_corpus(source):\n",
    "    data = pd.read_csv(os.path.join(ROOT_DIR,\"results_0\",f\"{source}_Results.csv\"))\n",
    "    data_keep = data.query(query_string_gen(source))\n",
    "    data_removed = pd.concat([data, data_keep])\n",
    "    data_removed = data_removed.drop_duplicates(keep=False)\n",
    "    data_keep = data_keep.drop(columns=[\"Topic\"])\n",
    "    data_keep.to_csv(os.path.join(ROOT_DIR,\"data_filtered\",f\"{source}.csv\"),index=False)\n",
    "    data_removed.to_csv(os.path.join(ROOT_DIR,\"data_filtered\",\"data_removed\",f\"{source}.csv\"),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = [\"ABC\",\"AP\",\"CBS\",\"CNN\",\"DailyMail\",\"Express\",\"Fox\",\"Guardian\", \"Mirror\", \"NYT\", \"Reuters\"] #Mirror, NYT, Reuters\n",
    "for corpus in corpora:\n",
    "    filter_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unite_sources():\n",
    "    total = 0\n",
    "    sources = [\"ABC\",\"AP\",\"CBS\",\"CNN\",\"DailyMail\",\"Express\",\"Fox\",\"Guardian\", \"Mirror\", \"NYT\", \"Reuters\"]\n",
    "    data = [pd.read_csv(os.path.join(ROOT_DIR,\"results_0\",f\"{i}_Results.csv\")) for i in sources]\n",
    "    result = pd.DataFrame(columns=[\"Source\", \"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    print(\"-> Current Dataset:\")\n",
    "    for i, j in zip(data, sources):\n",
    "        if j == \"All\":\n",
    "            continue\n",
    "        i[\"Source\"] = j\n",
    "        print(f\"{j}: {len(i)} Articles\")\n",
    "        total += len(i)\n",
    "        result = pd.concat([result, i])\n",
    "    result = result.drop_duplicates(subset=[\"Text\"])\n",
    "    result = result.set_index(\"Date\")\n",
    "    result = result.sort_index(ascending=False)\n",
    "    result = result.drop(columns=[\"Topic\"])\n",
    "    result.to_csv(os.path.join(ROOT_DIR, \"data_filtered\", \"All.csv\"), index=True)\n",
    "    print(f\"-> Saved CSV with {total} articles.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Current Dataset:\n",
      "ABC: 885 Articles\n",
      "AP: 5608 Articles\n",
      "CBS: 3306 Articles\n",
      "CNN: 2367 Articles\n",
      "DailyMail: 7575 Articles\n",
      "Express: 9607 Articles\n",
      "Fox: 4300 Articles\n",
      "Guardian: 4672 Articles\n",
      "Mirror: 2136 Articles\n",
      "NYT: 2387 Articles\n",
      "Reuters: 14825 Articles\n",
      "-> Saved CSV with 57668 articles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unite_sources()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (v3.10.5:f377153967, Jun  6 2022, 12:36:10) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71a723942456804a71d025442f2ccd3a5c8db2153e1c9e51f0af23a7e755532d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
