{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapers import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> CSV file found with 2712 articles! Latest article date: 2022-06-22\n",
      "-> Checking articles from latest date onward...\n",
      "-> API Query |████████████████████████████████████████| (!) 140 in 31.9s (4.39/s) \n",
      "-> 135 new articles saved to Guardian.csv! Total articles: 2847\n"
     ]
    }
   ],
   "source": [
    "guardianScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> CSV file found with 3111 articles! Latest article date: 20220622\n",
      "-> Checking articles from latest date onward...\n",
      "-> API Query |████████████████████████████████████████| (!) 245 in 2:53.9 (1.41/s) \n",
      "-> 205 new articles saved to NYT.csv! Total articles: 3316\n"
     ]
    }
   ],
   "source": [
    "NYTScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_data = pd.read_csv(PARENT_DIR + \"/data/Guardian.csv\")\n",
    "nyt_data = pd.read_csv(PARENT_DIR + \"/data/NYT.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword removal with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import gensim\n",
    "from spacy.lang.en import English\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_data = guardian_data.dropna(subset=['Text'])\n",
    "guardian_texts = guardian_data['Text'].tolist()\n",
    "guardian_no_sw = []\n",
    "for text in guardian_texts:\n",
    "    text_no_sw = remove_stopwords(text)\n",
    "    guardian_no_sw.append(text_no_sw) \n",
    "guardian_data['Text'] = guardian_no_sw\n",
    "\n",
    "nyt_data = nyt_data.dropna(subset=['Text'])\n",
    "nyt_texts = nyt_data['Text'].tolist()\n",
    "nyt_no_sw = []\n",
    "for text in nyt_texts:\n",
    "    text_no_sw = remove_stopwords(text)\n",
    "    nyt_no_sw.append(text_no_sw) \n",
    "nyt_data['Text'] = nyt_no_sw\n",
    "\n",
    "guardian_data.to_csv(PARENT_DIR + \"/data/Guardian_no_sw.csv\", index=False)\n",
    "nyt_data.to_csv(PARENT_DIR + \"/data/NYT_no_sw.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA topic identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from lda import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_topics(dataframe, topicnum):\n",
    "\n",
    "    texts = dataframe['Text'].tolist()\n",
    "\n",
    "    # Get vocabulary and word counts.  Use the top 10,000 most frequent\n",
    "    # lowercase unigrams with at least 3 alphabetical, non-numeric characters,\n",
    "    # punctuation treated as separators.\n",
    "    CVzer = CountVectorizer(token_pattern=r\"(?u)\\b[^\\W\\d]{3,}\\b\",\n",
    "                            max_features=None,\n",
    "                            lowercase=True)\n",
    "    doc_vcnts = CVzer.fit_transform(texts)\n",
    "    vocabulary = CVzer.get_feature_names_out()\n",
    "\n",
    "    # Learn topics.  Refresh conrols print frequency.\n",
    "    lda_model = LDA(topicnum, n_iter=8000, refresh=2000) \n",
    "    doc_topic = lda_model.fit_transform(doc_vcnts)\n",
    "    topic_word = lda_model.topic_word_\n",
    "\n",
    "    return doc_topic, topic_word, vocabulary\n",
    "\n",
    "doc_topic, topic_word, vocabulary = learn_topics(guardian_data, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_topicmodel(doc_topic, topic_word, vocabulary):\n",
    "\n",
    "    ## Topic mixtures.\n",
    "    topicmixture_outpath = PARENT_DIR + \"/results/GuardianTopicMixtures.txt\"\n",
    "    np.savetxt(topicmixture_outpath, doc_topic)\n",
    "\n",
    "    ## Topics.\n",
    "    topic_outpath = PARENT_DIR + \"/results/GuardianTopics.txt\"\n",
    "    np.savetxt(topic_outpath, topic_word)\n",
    "\n",
    "    ## Vocabulary order.\n",
    "    vocab_outpath = PARENT_DIR + \"/results/GuardianVocab.txt\"\n",
    "    with open(vocab_outpath, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "        for v in vocabulary:\n",
    "            f.write(v + \"\\n\")\n",
    "\n",
    "    return topicmixture_outpath, topic_outpath, vocab_outpath\n",
    "\n",
    "save_topicmodel(doc_topic, topic_word, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def KLdivergence_from_probdist_arrays(pdists0, pdists1):\n",
    "    \"\"\"\n",
    "    Calculate KL divergence between probability distributions held on the same\n",
    "    rows of two arrays.\n",
    "\n",
    "    NOTE: elements of pdist* are assumed to be positive (non-zero), a\n",
    "    necessary condition for using Kullback-Leibler Divergence.\n",
    "\n",
    "    Args:\n",
    "      pdists* (numpy.ndarray): arrays, where rows for each constitute the two\n",
    "      probability distributions from which to calculate divergence.  pdists1\n",
    "      contains the distributions holding probabilities in the numerator of the\n",
    "      KL divergence summand.\n",
    "\n",
    "    Returns:\n",
    "      numpy.ndarray: KL divergences, where the second array's rows are the\n",
    "        distributions in the numerator of the log in KL divergence\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert pdists0.shape == pdists1.shape, 'pdist* shapes must be identical'\n",
    "\n",
    "    if len(pdists0.shape) == 1:\n",
    "        KLdivs = (pdists1 * np.log2(pdists1/pdists0)).sum()\n",
    "    elif len(pdists0.shape) == 2:\n",
    "        KLdivs = (pdists1 * np.log2(pdists1/pdists0)).sum(axis=1)\n",
    "\n",
    "    return KLdivs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def novelty_transience_resonance(thetas_arr, scale):\n",
    "    \"\"\"\n",
    "    Calculate novelty, transience, and resonance for all center speeches with\n",
    "    at least one scale of speeches in its past and its future.  Presidential\n",
    "    speeches are excluded from the surrounding scales.\n",
    "    \n",
    "    Args:\n",
    "      thetas_arr (numpy.ndarray): rows are topic mixtures\n",
    "      scale (int): positive integer defining scale or scale size\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Find the first and last center speech offset, given scale size.\n",
    "    speechstart = scale\n",
    "    speechend = thetas_arr.shape[0] - scale\n",
    "\n",
    "    # Calculate novelty, transience, resonance.\n",
    "    novelties = []\n",
    "    transiences = []\n",
    "    resonances = []\n",
    "    for j in range(speechstart, speechend, 1):\n",
    "\n",
    "        center_theta = thetas_arr[j]\n",
    "\n",
    "        # Define windows before and after center speech.\n",
    "        after_boxend = j + scale + 1\n",
    "        before_boxstart = j - scale\n",
    "\n",
    "        before_theta_arr = thetas_arr[before_boxstart:j]\n",
    "        beforenum = before_theta_arr.shape[0]\n",
    "        before_centertheta_arr = np.tile(center_theta, reps=(beforenum, 1))\n",
    "\n",
    "        after_theta_arr = thetas_arr[j+1:after_boxend]\n",
    "        afternum = after_theta_arr.shape[0]\n",
    "        after_centertheta_arr = np.tile(center_theta, reps=(afternum, 1))\n",
    "\n",
    "        # Calculate KLDs.\n",
    "        before_KLDs = KLdivergence_from_probdist_arrays(before_theta_arr,\n",
    "                before_centertheta_arr)\n",
    "        after_KLDs = KLdivergence_from_probdist_arrays(after_theta_arr,\n",
    "                after_centertheta_arr)\n",
    "\n",
    "        # Calculate means of KLD.\n",
    "        novelty = np.mean(before_KLDs)\n",
    "        transience = np.mean(after_KLDs)\n",
    "\n",
    "        # Final measures for this center speech.\n",
    "        novelties.append(novelty)\n",
    "        transiences.append(transience)\n",
    "        resonances.append(novelty - transience)\n",
    "\n",
    "    return novelties, transiences, resonances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_novel_trans_reson(novelties, transiences, resonances):\n",
    "\n",
    "    outpath = PARENT_DIR + \"/results/GuardianNovelTransReson.txt\"\n",
    "    np.savetxt(outpath, np.vstack(zip(novelties, transiences, resonances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelties, transiences, resonances = novelty_transience_resonance(doc_topic, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_novel_trans_reson(novelties, transiences, resonances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bd6d5ee84073ea568a8475b6416df4df35e9f8587352d1cb55f8675bca2c3cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
